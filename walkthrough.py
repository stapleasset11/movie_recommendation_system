# -*- coding: utf-8 -*-
"""walkthrough.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uad29-wblZzE_UI2c41Y66rCABYzIi1x

##1. Import Dependencies.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""##2. Load the Dataset."""

ratings = pd.read_csv("https://s3-us-west-2.amazonaws.com/recommender-tutorial/ratings.csv")
movies = pd.read_csv("https://s3-us-west-2.amazonaws.com/recommender-tutorial/movies.csv")

ratings.head()

movies.head()

"""##3. Explore your Data."""

n_ratings = len(ratings)
n_movies = ratings['movieId'].nunique()
n_users = ratings['userId'].nunique()

print(f"Number of ratings: {n_ratings}")
print(f"Number of unique movieId's: {n_movies}")
print(f"Number of unique users: {n_users}")
print(f"Average number of ratings per user: {round(n_ratings/n_users, 2)}")
print(f"Average number of ratings per movie: {round(n_ratings/n_movies, 2)}")

"""##4. Ratings Distribution."""

sns.countplot(x='rating',data=ratings, palette='husl')
plt.title("Distribution of movie ratings", fontsize=15)
plt.show()

print(f"Mean global ratings: {round(ratings['rating'].mean(),2)}.")

mean_ratings = ratings.groupby('userId')['rating'].mean()
print(f"Mean ratings per user: {round(mean_ratings.mean(),2)}.")

mean_ratings_per_movie = ratings.groupby('movieId')['rating'].mean()
print(f"Mean ratings per movie: {round(mean_ratings_per_movie.mean(),2)}.")

"""##5.Which movies are most frequently rated."""

ratings['movieId'].value_counts()

movie_ratings = ratings.merge(movies, on='movieId')
movie_ratings['title'].value_counts()[0:20]

"""##6. What are the lowest and highest rated movies.

###6.1. Lowest rated movie.
"""

mean_ratings = ratings.groupby('movieId')[['rating']].mean()
lowest_rated  =mean_ratings['rating'].idxmin()


movies[movies['movieId'] == lowest_rated]

"""###6.2. Highest rated movie."""

mean_ratings = ratings.groupby('movieId')[['rating']].mean()
highest_rated = mean_ratings['rating'].idxmax()

movies[movies['movieId'] == highest_rated]

"""####6.2.1. Lets dig deeper.
**Bayesian Average**
* Bayesian Average is defined as:

     *r1* = (C * *m*) + sum(reviews) / C + N

     where:
     * C -> Dataset size.
     * *m* -> average ratings


"""

movie_stats = ratings.groupby('movieId')['rating'].agg(['count','mean'])
C = movie_stats['count'].mean()
m = movie_stats['mean'].mean()

print(f"Average number of ratings per given movie: {C:.2f}.")
print(f"Average rating for a single movie: {m:.2f}")

def bayesian_avg(ratings):
  bayesian_avg = ((C * m) + (ratings.sum())) / (C + (ratings.count()))

  return round(bayesian_avg, 3)

lamerica = pd.Series([5,5])
bayesian_avg(lamerica)

bayesian_avg_ratings = ratings.groupby('movieId')['rating'].agg(bayesian_avg).reset_index()
bayesian_avg_ratings.columns = ['movieId','bayesian_avg']
movie_stats = movie_stats.merge(bayesian_avg_ratings, on='movieId')
movie_stats

movie_stats = movie_stats.merge(movies[['movieId', 'title']])
movie_stats.sort_values(by='bayesian_avg', ascending=False)[0:5]

"""##7. Data cleaning."""

movies.head()

movies['genres'] = movies['genres'].apply(lambda x: x.split('|'))
movies.head()

"""##8. How many movie genres are there"""

from collections import Counter
genre_frequency = Counter(e for genres in movies['genres'] for e in genres)

print(f"There are {len(genre_frequency)} genres.")
print(f"The 5 most common genres are: {genre_frequency.most_common(5)}")
genre_frequency

"""###8.1. Let's Plot the Genre Frequency to get a better idea"""

genre_frequency_df = pd.DataFrame([genre_frequency]).T.reset_index()
genre_frequency_df.columns = ['genre','count']

sns.barplot(x = 'genre', y='count',data=genre_frequency_df.sort_values(by='count',ascending=False),palette='viridis')
plt.xticks(rotation=90)

"""##9. Data Preprocessing."""

from scipy.sparse import csr_matrix

def create_X(df):
    """
    Generates a sparse matrix from ratings dataframe.

    Args:
        df: pandas dataframe containing 3 columns (userId, movieId, rating)

    Returns:
        X: sparse matrix
        user_mapper: dict that maps user id's to user indices
        user_inv_mapper: dict that maps user indices to user id's
        movie_mapper: dict that maps movie id's to movie indices
        movie_inv_mapper: dict that maps movie indices to movie id's
    """
    M = df['userId'].nunique()
    N = df['movieId'].nunique()

    user_mapper = dict(zip(np.unique(df["userId"]), list(range(M))))
    movie_mapper = dict(zip(np.unique(df["movieId"]), list(range(N))))

    user_inv_mapper = dict(zip(list(range(M)), np.unique(df["userId"])))
    movie_inv_mapper = dict(zip(list(range(N)), np.unique(df["movieId"])))

    user_index = [user_mapper[i] for i in df['userId']]
    item_index = [movie_mapper[i] for i in df['movieId']]

    X = csr_matrix((df["rating"], (user_index,item_index)), shape=(M,N))

    return X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper

X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper = create_X(ratings)

X.shape
#user_mapper

"""##10. Checking for sparcity.
* Sparcity is the property of being scanty or scattered; lacking denseness.

* Anything above 1% is okay but below 1% then we consider content based filtering.
"""

n_total = X.shape[0] * X.shape[1]
n_ratings = X.nnz
sparcity = n_ratings / n_total
print(f"Matrix sparcity: {round(sparcity*100,2)}% . ")

"""`csr_matrix.nn` counts the stored values in our sparse matrix. The rest of the cells are empty.

The `cold start problem` is when there are new users and movies that do not have ratings. In our Movies Dataset all movies have at least one rating but in general it's useful to check which users and movies have few interactions.
"""

n_ratings_per_user = X.getnnz(axis=1)
print(f"Number of ratings per user: {len(n_ratings_per_user)}.")

n_ratings_per_movie = X.getnnz(axis=0)
print(f"Number of ratings per movie: {len(n_ratings_per_movie)}.")

print(f"The most rated movie has: {n_ratings_per_movie.max()} ratings")
print(f"The least rated movie has: {n_ratings_per_movie.min()} ratings")

print(f"Most active user has rated: {n_ratings_per_user.max()} movies.")
print(f"Least active user has rated: {n_ratings_per_user.min()} movies.")

plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.kdeplot(n_ratings_per_user, fill=True)
plt.xlim(0)
plt.title("Number of Ratings Per User", fontsize=14)
plt.xlabel("Number of Ratings Per User")
plt.ylabel("Density")
plt.subplot(1,2,2)
sns.kdeplot(n_ratings_per_movie, fill=True)
plt.xlim(0)
plt.title("Number of Ratings Per Movie", fontsize=14)
plt.xlabel("Number of Ratings Per Movie")
plt.ylabel("Density")
plt.show()

"""##11. Item - Item Recommendation with K-nearest Neighbors"""

from sklearn.neighbors import NearestNeighbors

def find_similar_movies(movie_id, X, movie_mapper, movie_inv_mapper, k, metric='cosine'):
    """
    Finds k-nearest neighbours for a given movie id.

    Args:
        movie_id: id of the movie of interest
        X: user-item utility matrix
        k: number of similar movies to retrieve
        metric: distance metric for kNN calculations

    Output: returns list of k similar movie ID's
    """
    X = X.T
    neighbour_ids = []

    movie_ind = movie_mapper[movie_id]
    movie_vec = X[movie_ind]
    if isinstance(movie_vec, (np.ndarray)):
        movie_vec = movie_vec.reshape(1,-1)
    # use k+1 since kNN output includes the movieId of interest
    kNN = NearestNeighbors(n_neighbors=k+1, algorithm="brute", metric=metric)
    kNN.fit(X)
    neighbour = kNN.kneighbors(movie_vec, return_distance=False)
    for i in range(0,k):
        n = neighbour.item(i)
        neighbour_ids.append(movie_inv_mapper[n])
    neighbour_ids.pop(0)
    return neighbour_ids

similar_movies = find_similar_movies(1, X, movie_mapper, movie_inv_mapper, k=10)
similar_movies

movie_titles = dict(zip(movies['movieId'], movies['title']))

movie_id = 1

similar_movies = find_similar_movies(movie_id, X, movie_mapper, movie_inv_mapper, metric='cosine', k=10)
movie_title = movie_titles[movie_id]

print(f"Because you watched {movie_title}:\n")
for i in similar_movies:
  print(movie_titles[i])

"""##12. Dealing with the `cold start problem`.

* `Collaborative filtering` relies solely on user-item interactions within the utility matrix. The issue with this approach is that brand new users or items with no iteractions get excluded from the recommendation system. This is called the cold start problem. Content-based filtering is a way to handle this problem by generating recommendations based on user and item features.
"""

n_movies = movies['movieId'].nunique()
print(f"There are {n_movies} unique movies in this dataset.")

genres = set(g for G in movies['genres'] for g in G)
for g in genres:
  movies[g] = movies.genres.transform(lambda x: int(g in x))

movie_genres = movies.drop(columns=['movieId','title','genres'])
movie_genres

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(movie_genres, movie_genres)
print(f"Dimensions of our genres cosine similarity matrix: {cosine_sim.shape}")

from fuzzywuzzy import process

def movie_finder(title):
  all_titles = movies['title'].tolist()
  closest_match = process.extractOne(title,all_titles)
  return closest_match[0]

title = movie_finder('juminji')
title

movie_idx = dict(zip(movies['title'], list(movies.index)))
idx = movie_idx[title]
print(f"Movie index for Jumanji: {idx}")

n_recommendations = 10
sim_scores = list(enumerate(cosine_sim[idx]))
sim_scores = sorted(sim_scores,key=lambda x: x[1], reverse=True)
sim_scores = sim_scores[1: (n_recommendations+1)]
sim_scores

similar_movies = [i[0] for i in sim_scores]
similar_movies

"""`similar_movies` is an array of indices that represents Jumanji's top 10 recommendations. We can get the corresponding movie titles by either creating an inverse movie_idx mapper or using iloc on the title column of the movies dataframe."""

print(f"Because you watched {title}:")
movies['title'].iloc[similar_movies]

"""#** Let's put it all Together"""

def get_content_based_recommendations(title_string, n_recommendations=10):
    title = movie_finder(title_string)
    idx = movie_idx[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:(n_recommendations+1)]
    similar_movies = [i[0] for i in sim_scores]
    print(f"Because you watched {title}:")
    print(movies['title'].iloc[similar_movies])

get_content_based_recommendations('Jumaji', 5)

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components= 20, n_iter=10)
Q = svd.fit_transform(X.T)
Q.shape

# movie_id = ___________
# similar_movies = find_similar_movies(movie_id,Q.T, movie_mapper, movie_inv_mapper, metric='cosine', k=10)
# movie_title = movie_titles[movie_id]

# print(f"Because you watched {movie_title}:")
# for i in similar_movies:
#     print(movie_titles[i])

